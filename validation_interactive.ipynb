{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4d8144-e1bc-4d8f-9dcf-c1af2b23c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ClassLabel, load_from_disk\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForAudioClassification\n",
    "import transformers\n",
    "from utils.dataset_utils import CremaDataset\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from fairlearn.metrics import MetricFrame, demographic_parity_difference, equalized_odds_difference\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f325ff07-111b-4f11-bdfd-1e1d19e83211",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84aa358d-c0c7-46ac-9271-2e3d86e4c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "crema_dataset = CremaDataset(\"distil-whisper/distil-medium.en\")\n",
    "crema_dataset.set_vector(\"actor-vector.hf\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b45ef94-ee75-461b-ac31-633fec3d6f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at distil-whisper/distil-medium.en and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "torch.manual_seed(42)\n",
    "test_dataloader = DataLoader(crema_dataset.test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "num_labels = len(crema_dataset.id2label)\n",
    "\n",
    "transformers.set_seed(42)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    crema_dataset.model_id, num_labels=num_labels, label2id=crema_dataset.label2id, id2label=crema_dataset.id2label\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6b89f0b-e31e-4107-bd87-ee42533789e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:m74593jd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">evaluation_run</strong> at: <a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/m74593jd' target=\"_blank\">https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/m74593jd</a><br/> View project at: <a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness' target=\"_blank\">https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241104_003525-m74593jd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:m74593jd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Crema-Emotion/wandb/run-20241104_003914-wq8f0ma8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/wq8f0ma8' target=\"_blank\">evaluation_run</a></strong> to <a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness' target=\"_blank\">https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/wq8f0ma8' target=\"_blank\">https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/wq8f0ma8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS=1\n",
    "\n",
    "num_training_steps = EPOCHS * len(test_dataloader)\n",
    "\n",
    "wandb.init(project=\"crema_evaluation_with_fairness\", \n",
    "           name=\"evaluation_run\",\n",
    "           config={})\n",
    "\n",
    "validation_results = {\"ActorID\": [], \"true_labels\": [], \"predictions\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbf3e438-c3c8-46c1-883d-8cd35fc64c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"model_latest.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5f786c4-e8fe-41a0-bae6-758c5f3e44fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9cfcf8d8854849a29bff11a48a298a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    actor_ids = [np.int64(id) for id in batch[\"path\"]]\n",
    "    true_labels_batch = batch[\"labels\"].numpy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = {\"input_features\": batch[\"input_features\"].to(device), \"labels\": batch[\"labels\"].to(device)}\n",
    "        outputs = model(**inputs)\n",
    "        preds = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "    # Append results to dictionary\n",
    "    validation_results[\"ActorID\"].extend(actor_ids)\n",
    "    validation_results[\"true_labels\"].extend(true_labels_batch)\n",
    "    validation_results[\"predictions\"].extend(preds)\n",
    "    \n",
    "    progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a383ea4-c28d-432c-8d77-bd242439c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(validation_results)\n",
    "\n",
    "demographics_path =  \"VideoDemographics.csv\"\n",
    "demographics_df = pd.read_csv(demographics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60c00591-6832-432e-a6c3-7ccc4b5586c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActorID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>51</td>\n",
       "      <td>Male</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>21</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>21</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>African American</td>\n",
       "      <td>Not Hispanic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ActorID  Age     Sex              Race     Ethnicity\n",
       "0     1001   51    Male         Caucasian  Not Hispanic\n",
       "1     1002   21  Female         Caucasian  Not Hispanic\n",
       "2     1003   21  Female         Caucasian  Not Hispanic\n",
       "3     1004   42  Female         Caucasian  Not Hispanic\n",
       "4     1005   29    Male  African American  Not Hispanic"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0f60a26-27da-4247-9ade-25c3bb73445e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActorID</th>\n",
       "      <th>true_labels</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1031</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1035</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1044</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1053</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ActorID  true_labels  predictions\n",
       "0     1031            3            3\n",
       "1     1004            3            3\n",
       "2     1035            0            0\n",
       "3     1044            1            4\n",
       "4     1053            2            4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa65a2d0-4b83-4f1f-bf26-12c757898214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.int64, numpy.int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results_df.loc[0, \"ActorID\"]), type(demographics_df.loc[0, \"ActorID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f30103d7-3a6e-4d75-9176-569c660f0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictions with demographic info using actor ID\n",
    "merged_data = results_df.merge(demographics_df, on=\"ActorID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb40879c-6dc4-44c8-be7d-42a3370f1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = merged_data[\"true_labels\"]\n",
    "predictions = merged_data[\"predictions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3306bcf7-d677-43bd-bff2-38fadf02f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "report = classification_report(true_labels, predictions, output_dict=True)\n",
    "wandb.log({\"classification_report\": report})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cf4d587-dd4e-410b-9a79-f7415de3fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "wandb.log({\"confusion_matrix\": wandb.Image(fig)})\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "934f9f7f-90fe-47d0-8803-82a67090a307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.metrics._classification.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b2523e3-3603-4abc-b5f9-e726b740c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_features = merged_data[\"Race\"]  # Replace with demographic column\n",
    "metric_frame = MetricFrame(\n",
    "    metrics={\n",
    "        'accuracy': accuracy_score,\n",
    "        'f1_score': lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted', pos_label=1)\n",
    "    },\n",
    "    y_true=list(true_labels),\n",
    "    y_pred=list(predictions),\n",
    "    sensitive_features=list(sensitive_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4cd9f473-9704-43b3-8b68-4236e53b5464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022237344346411364"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_parity_diff = demographic_parity_difference(\n",
    "    true_labels, predictions, sensitive_features=sensitive_features\n",
    ")\n",
    "demographic_parity_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e74701b6-08f5-4dd7-92fc-41dcef56abff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 0, 1, 2] [3, 3, 0, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "print(list(true_labels)[:5], list(predictions)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1bfb79d-457a-48da-a4bb-73f6520ea934",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If pos_label is not specified, values must be from {0, 1} or {-1, 1}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m equalized_odds_diff \u001b[38;5;241m=\u001b[39m \u001b[43mequalized_odds_difference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msensitive_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairlearn/metrics/_fairness_metrics.py:155\u001b[0m, in \u001b[0;36mequalized_odds_difference\u001b[0;34m(y_true, y_pred, sensitive_features, method, sample_weight, agg)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworst_case\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg must be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworst_case\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m eo \u001b[38;5;241m=\u001b[39m \u001b[43m_get_eo_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agg \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworst_case\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(eo\u001b[38;5;241m.\u001b[39mdifference(method\u001b[38;5;241m=\u001b[39mmethod))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairlearn/metrics/_fairness_metrics.py:228\u001b[0m, in \u001b[0;36m_get_eo_frame\u001b[0;34m(y_true, y_pred, sensitive_features, sample_weight)\u001b[0m\n\u001b[1;32m    226\u001b[0m sw_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight}\n\u001b[1;32m    227\u001b[0m sp \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpr\u001b[39m\u001b[38;5;124m\"\u001b[39m: sw_dict, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfpr\u001b[39m\u001b[38;5;124m\"\u001b[39m: sw_dict}\n\u001b[0;32m--> 228\u001b[0m eo \u001b[38;5;241m=\u001b[39m \u001b[43mMetricFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43msensitive_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msensitive_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eo\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairlearn/metrics/_metric_frame.py:274\u001b[0m, in \u001b[0;36mMetricFrame.__init__\u001b[0;34m(self, metrics, y_true, y_pred, sensitive_features, control_features, sample_params, n_boot, ci_quantiles, random_state)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Create the basic results\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mDisaggregatedResult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotated_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotated_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43msensitive_feature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sf_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_feature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cf_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# Build into cache\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_populate_results(result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairlearn/metrics/_disaggregated_result.py:356\u001b[0m, in \u001b[0;36mDisaggregatedResult.create\u001b[0;34m(data, annotated_functions, sensitive_feature_names, control_feature_names)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Calculate the 'overall' values\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m control_feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 356\u001b[0m     overall \u001b[38;5;241m=\u001b[39m \u001b[43mapply_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotated_functions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     temp \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(by\u001b[38;5;241m=\u001b[39mcontrol_feature_names)\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    359\u001b[0m         apply_to_dataframe,\n\u001b[1;32m    360\u001b[0m         metric_functions\u001b[38;5;241m=\u001b[39mannotated_functions,\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;66;03m# See note in apply_to_dataframe about include_groups\u001b[39;00m\n\u001b[1;32m    362\u001b[0m         include_groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    363\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairlearn/metrics/_disaggregated_result.py:57\u001b[0m, in \u001b[0;36mapply_to_dataframe\u001b[0;34m(data, metric_functions, include_groups)\u001b[0m\n\u001b[1;32m     55\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m function_name, metric_function \u001b[38;5;129;01min\u001b[39;00m metric_functions\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 57\u001b[0m     values[function_name] \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# correctly handle zero provided metrics\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairlearn/metrics/_annotated_metric_function.py:106\u001b[0m, in \u001b[0;36mAnnotatedMetricFunction.__call__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func_arg_name, data_arg_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_argument_mapping\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# Need to convert to list first in case we have 2D arrays\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     kwargs[func_arg_name] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mlist\u001b[39m(df[data_arg_name]))\n\u001b[0;32m--> 106\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairlearn/metrics/_base_metrics.py:110\u001b[0m, in \u001b[0;36mtrue_positive_rate\u001b[0;34m(y_true, y_pred, sample_weight, pos_label)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrue_positive_rate\u001b[39m(y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the true positive rate (also called sensitivity, recall, or hit rate).\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <custom_fairness_metrics>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        The true positive rate for the data\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     unique_labels \u001b[38;5;241m=\u001b[39m \u001b[43m_get_labels_for_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     tnr, fpr, fnr, tpr \u001b[38;5;241m=\u001b[39m skm\u001b[38;5;241m.\u001b[39mconfusion_matrix(\n\u001b[1;32m    112\u001b[0m         y_true,\n\u001b[1;32m    113\u001b[0m         y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m         normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    117\u001b[0m     )\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tpr\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fairlearn/metrics/_base_metrics.py:63\u001b[0m, in \u001b[0;36m_get_labels_for_confusion_matrix\u001b[0;34m(labels, pos_label)\u001b[0m\n\u001b[1;32m     61\u001b[0m         pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_RESTRICTED_VALS_IF_POS_LABEL_NONE)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Ensure unique_labels has two elements\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_labels) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: If pos_label is not specified, values must be from {0, 1} or {-1, 1}"
     ]
    }
   ],
   "source": [
    "equalized_odds_diff = equalized_odds_difference(\n",
    "    list(true_labels), list(predictions), sensitive_features=list(sensitive_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e30f925-07c8-4220-b2c6-e888e28dfe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_diff = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4a48c049-e36a-458f-b190-e03ca03072f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final metrics and fairness assessment logged.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">evaluation_run</strong> at: <a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/wq8f0ma8' target=\"_blank\">https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/wq8f0ma8</a><br/> View project at: <a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness' target=\"_blank\">https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241104_003914-wq8f0ma8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({\n",
    "    \"fairness_metrics\": {\n",
    "        \"demographic_parity_difference\": demographic_parity_diff,\n",
    "        \"equalized_odds_difference\": equalized_odds_diff\n",
    "    },\n",
    "    \"group_metrics\": metric_frame.by_group.to_dict()\n",
    "})\n",
    "\n",
    "print(\"Final metrics and fairness assessment logged.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2e9d3-bffb-48e2-87da-bbedb62acf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
