{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "323f933d-be87-4cee-9cd7-b86ca249227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoFeatureExtractor\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from fairlearn.metrics import MetricFrame, demographic_parity_difference, equalized_odds_difference\n",
    "# import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from transformers import AutoModelForAudioClassification\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from utils.dataset_utils import CremaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30c7d39-d86b-46c9-98e9-0d594b99bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install fairlearn\n",
    "# pip3 install tqdm\n",
    "# pip3 install seaborn\n",
    "# pip3 install wandb\n",
    "# pip3 install datasets\n",
    "# pip3 install transformers\n",
    "# pip3 install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85aac60b-451b-4b03-8692-d1af486209a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maxhero7\u001b[0m (\u001b[33maxhero7-organization\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba9372591994031b9692fb24dd528eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011122461315244437, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Crema-Emotion/wandb/run-20241101_015740-wtnb36i4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/wtnb36i4' target=\"_blank\">evaluation_run</a></strong> to <a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness' target=\"_blank\">https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/wtnb36i4' target=\"_blank\">https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/wtnb36i4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/axhero7-organization/crema_evaluation_with_fairness/runs/wtnb36i4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x731f83a41660>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"crema_evaluation_with_fairness\", name=\"evaluation_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b7354b-fe5f-4f74-825b-752c78254c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810dd38d-fa24-4044-ae7e-da6d8907e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"myleslinder/crema-d\", trust_remote_code=True, split='train')\n",
    "id2label = {'0': 'neutral', '1': 'happy', '2': 'sad', '3': 'anger', '4': 'fear', '5': 'disgust'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Select necessary columns\n",
    "dataset = dataset.select_columns(['audio', 'label'])\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "model_id = \"distil-whisper/distil-medium.en\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id, do_normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7dc0fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CremaDataset.__init__() missing 1 required positional argument: 'model_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCremaDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dataset\u001b[38;5;241m.\u001b[39mset_vector(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactor-vector.hf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: CremaDataset.__init__() missing 1 required positional argument: 'model_id'"
     ]
    }
   ],
   "source": [
    "model_id = \"distil-whisper/distil-medium.en\"\n",
    "dataset = CremaDataset(model_id)\n",
    "dataset.set_vector(\"actor-vector.hf\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b9dfcb3-a2e6-4660-838d-968cf7236131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActorID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>51</td>\n",
       "      <td>Male</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>21</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>21</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Not Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>African American</td>\n",
       "      <td>Not Hispanic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ActorID  Age     Sex              Race     Ethnicity\n",
       "0     1001   51    Male         Caucasian  Not Hispanic\n",
       "1     1002   21  Female         Caucasian  Not Hispanic\n",
       "2     1003   21  Female         Caucasian  Not Hispanic\n",
       "3     1004   42  Female         Caucasian  Not Hispanic\n",
       "4     1005   29    Male  African American  Not Hispanic"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_path = \"VideoDemographics.csv\"\n",
    "demographics = pd.read_csv(demographics_path)\n",
    "demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c80d566-4e71-4b8b-9497-8853199e91a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at distil-whisper/distil-medium.en and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_id, num_labels=len(id2label))\n",
    "model.to(device)\n",
    "\n",
    "# Load pre-trained model weights\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2bab56d-e7a1-4216-b66d-87b9069f9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, batch, device):\n",
    "    inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=-1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "886d07be-009b-4c40-b63a-91d99bb0ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(4), 'input_features': tensor([[ 0.3249,  0.2401,  0.3034,  ..., -1.0937, -1.0937, -1.0937],\n",
      "        [ 0.3307,  0.2953,  0.2271,  ..., -1.0937, -1.0937, -1.0937],\n",
      "        [ 0.3051,  0.3750,  0.0698,  ..., -1.0937, -1.0937, -1.0937],\n",
      "        ...,\n",
      "        [-0.5717, -0.6035, -0.5495,  ..., -1.0937, -1.0937, -1.0937],\n",
      "        [-0.4953, -0.5553, -0.4541,  ..., -1.0937, -1.0937, -1.0937],\n",
      "        [-0.5559, -0.5777, -0.5451,  ..., -1.0937, -1.0937, -1.0937]]), 'path': '1061'}\n",
      "{'input_features': tensor([[[ 0.3249,  0.2401,  0.3034,  ..., -1.0937, -1.0937, -1.0937],\n",
      "         [ 0.3307,  0.2953,  0.2271,  ..., -1.0937, -1.0937, -1.0937],\n",
      "         [ 0.3051,  0.3750,  0.0698,  ..., -1.0937, -1.0937, -1.0937],\n",
      "         ...,\n",
      "         [-0.5717, -0.6035, -0.5495,  ..., -1.0937, -1.0937, -1.0937],\n",
      "         [-0.4953, -0.5553, -0.4541,  ..., -1.0937, -1.0937, -1.0937],\n",
      "         [-0.5559, -0.5777, -0.5451,  ..., -1.0937, -1.0937, -1.0937]]],\n",
      "       device='cuda:0')}\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([4, 0, 1,  ..., 3, 3, 3])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(preds)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n\u001b[0;32m---> 11\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmake_prediction\u001b[49m(model, batch, device)\n\u001b[1;32m     13\u001b[0m actor_ids \u001b[38;5;241m=\u001b[39m [dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]))]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(actor_ids)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "# for batch in tqdm(dataset['train'], desc=\"Evaluating\"):\n",
    "for batch in range(0, len(dataset.test_dataset)):\n",
    "    print(dataset.test_dataset[batch])\n",
    "    inputs = {\"input_features\": dataset.test_dataset[batch][\"input_features\"].unsqueeze(0).to(device)}\n",
    "    labels = dataset.test_dataset[\"labels\"]\n",
    "    print(inputs)\n",
    "    outputs = model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=-1)\n",
    "    print(preds)\n",
    "    print(labels)\n",
    "    preds = make_prediction(model, batch, device)\n",
    "    \n",
    "   \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22402087-15af-45a0-9044-fce46c9a9b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1059'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]['audio']['path'].split('/')[-1][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73bf1cc9-2ea8-4a26-85bc-ea129687a7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': [{'path': '/root/.cache/huggingface/datasets/downloads/extracted/a5b997a2046c58b60a966d6760b86c2830b98882fce95b7c3a0658d7aa4bd7d5/data/AudioWAV/1059_IOM_FEA_XX.wav',\n",
       "   'array': array([-0.00552368, -0.00549316, -0.00460815, ...,  0.        ,\n",
       "           0.        ,  0.        ]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/root/.cache/huggingface/datasets/downloads/extracted/a5b997a2046c58b60a966d6760b86c2830b98882fce95b7c3a0658d7aa4bd7d5/data/AudioWAV/1035_TIE_SAD_XX.wav',\n",
       "   'array': array([0.00585938, 0.0055542 , 0.00491333, ..., 0.        , 0.        ,\n",
       "          0.        ]),\n",
       "   'sampling_rate': 16000}],\n",
       " 'label': [4, 2]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e84ff15d-ff50-4655-a505-c258d844c4c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtest_dataloader\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "b = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edf2fab1-9113-4d80-894c-161ff3d33e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {key: val.to(device) for key, val in b.items()}\n",
    "labels = inputs.pop(\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb2ed652-6b2a-4da5-a2c7-281e42dbeb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 5], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6e10f541-9adc-40fe-9ab0-c517fd350d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80, 3000])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6a064ae2-7c3a-491e-8b37-bf77cb6ed7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3000])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['input_features'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "800288f8-f996-45fe-b8f5-5ca816a1c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0055, -0.0055, -0.0046,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(test_dataset)):\n",
    "    print(torch.tensor(dataset['test']['audio'][i]['array']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c772bd1-aceb-4c64-bbd7-393efba9809b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
